import sys
# import libraries

import pickle
import nltk
nltk.download(['punkt', 'wordnet'])

from sqlalchemy import create_engine
import pandas as pd
import numpy as np
from nltk.tokenize import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.multioutput import MultiOutputClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV


def load_data(database_filepath):
#     engine = create_engine('sqlite:///InsertDatabaseName.db')
    '''
    load load from the SQL file from database_filepath
    '''
    engine = create_engine('sqlite:///'+ database_filepath)
    df = pd.read_sql_table('df', engine, index_col=0)
    X = df['message'] #The input would be the texts from "message"
    Y = df[df.columns[4:-1]]     #The output is a multi-label vector 
    category_names = Y.columns
    return X, Y,category_names

def tokenize(text):
    #Write a tokenization function to process your text data
    tokens = word_tokenize(text)
    lemmatizer = WordNetLemmatizer()

    clean_tokens = []
    for tok in tokens:
        clean_tok = lemmatizer.lemmatize(tok).lower().strip()
        clean_tokens.append(clean_tok)

    return clean_tokens


def build_model():
    '''
    3. Build a machine learning pipeline
    This machine pipeline takes in the message column as input and output classification results on the other 36 categories in the dataset. 
    The MultiOutputClassifier is helpful for predicting multiple target variables.  
    GridSearchCV is used to find the optimal set of parameters. 
    '''
    
    vect = CountVectorizer(tokenizer=tokenize)
    tfidf = TfidfTransformer()
    # The parameters in the pipeline is obtained by GridSearch trained in the Jupyter Notebookï¼›
    # The script took too long to execute with optimal parameters specified
    pipeline = Pipeline([('vect',CountVectorizer(tokenizer=tokenize)),
                         ('tfidf',TfidfTransformer()),
                         ('MOC', MultiOutputClassifier(RandomForestClassifier()))])
#     criterion= 'gini',n_estimators= 100,max_features=5000 
    parameters = {
    'MOC__estimator__n_estimators': [50,100],
    'MOC__estimator__criterion':['gini','entropy'] #MOC__criterion
    }

    cv =  GridSearchCV(pipeline, parameters,  verbose=3,  scoring='f1_micro', n_jobs=-1) #scoring='f1_micro',
    return cv
#     return pipeline

def evaluate_model(model, X_test, Y_test, category_names):
    '''
    Report the f1 score, precision and recall for each output category of the dataset. 
    A classification report is print out for all columns together.
    A detailed result is generated by iterating through the columns and calling sklearn's classification_report on each.
    '''
    predicted=model.predict(X_test)
    print(classification_report(Y_test.values, predicted, target_names=Y_test.columns.values))
    result={}
    for i in range(predicted.shape[1]):
        result['cat_'+str(i)]=classification_report(Y_test[Y_test.columns[i]], predicted[:,i])
    return result


def save_model(model, model_filepath):
#     filename = 'model.pkl'
    pickle.dump(model, open(model_filepath, 'wb'))

def main():
    if len(sys.argv) == 3:
        database_filepath, model_filepath = sys.argv[1:]
        print('Loading data...\n    DATABASE: {}'.format(database_filepath))
        X, Y, category_names = load_data(database_filepath)
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
        
        print('Building model...')
        model = build_model()
        
        print('Training model...')
        model.fit(X_train, Y_train)
        
        print('Evaluating model...')
        evaluate_model(model, X_test, Y_test, category_names)

        print('Saving model...\n    MODEL: {}'.format(model_filepath))
        save_model(model, model_filepath)

        print('Trained model saved!')

    else:
        print('Please provide the filepath of the disaster messages database '\
              'as the first argument and the filepath of the pickle file to '\
              'save the model to as the second argument. \n\nExample: python '\
              'train_classifier.py ../data/DisasterResponse.db classifier.pkl')


if __name__ == '__main__':
    main()